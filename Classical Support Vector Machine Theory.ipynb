{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classical Suport Vector Machine\n",
    "We are using classsical support vector machine as a binary classifier. This notebook serves a theoritical background of support vector machine(SVM). The implementation is in the another notebook named: 'Classical SVM implementation'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVMs as Linear Classifiers <a id=\"linear\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/494px-SVM_margin.png)\n",
    "Source: [wikipedia](https://en.wikipedia.org/wiki/Support_vector_machine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our input dataset is of the form $(\\vec{x}_{1}, y_{1}), ..., (\\vec{x}_{n}, y_{n})$, \n",
    "where, $\\vec{x}$ is a $d$ dimensional vector where $d$ is the number of features and $y_{i}$'s are the labels $ y_{i} \\in {-1, +1}$ as it is a binary classification problem.\n",
    "\n",
    "$\\vec{w}$ is a vector perpendicular to the **decision boundary** (hyperplane that cuts the space into two parts and is the result of classification). Let $\\vec{u}$ be a vector representing a point on our feature space. Now, to understand whether a point is on the +ve side or the -ve side we'll have to project the point $\\vec{u}$ onto $\\vec{w}$, which will give us a scaled version of $\\vec{u}$'s projection in the direction perpendicular to the decision boundary. Depending on the value of this quantity we'll have the point either on the +ve side or the -ve side. This can be represented mathematically as\n",
    "\n",
    "\n",
    "$$\\begin{equation} \\vec{w}\\cdot\\vec{x}_{+} + b \\geq 1 \\label{eq:vector_ray} \\tag{1}\\end{equation}$$ \n",
    "$$\\begin{equation} \\vec{w}\\cdot\\vec{x}_{-} + b \\leq -1  \\tag{2}\\end{equation}$$ \n",
    "\n",
    "where, $\\vec{x}_{+}$ is a datapoint with label $y_{i} = +1$,<br>\n",
    "$\\vec{x}_{-}$ is a datapoint with label $y_{i} = -1$ and<br>\n",
    "      b is parameter that has to be learnt\n",
    "\n",
    "These two lines are separated by a distance of $\\frac{2}{||{\\vec{w}}||}$. The line in the middle of both of these, i.e, \n",
    "\n",
    "$$\\begin{equation} \\vec{w}\\cdot\\vec{u} + b = 0  \\tag{3}\\end{equation}$$ \n",
    "\n",
    "is the equation of the hyperplane denoting our decision boundary. Together, the space between (1) and (2) forms what is usually know as the **street** or the **gutter**.\n",
    "\n",
    "equation (1) and (2) can be conveniently combined to give\n",
    "\n",
    "$$y_{i}(\\vec{w}\\cdot\\vec{x}_{i} + b) \\geq 1\\tag{4}$$\n",
    "\n",
    "And the limiting case would be \n",
    "\n",
    "$$y_{i}(\\vec{w}\\cdot\\vec{x}_{i} + b) -1 = 0 \\tag{5}$$\n",
    "\n",
    "Which is attained when the points lie on the edges of the street, i.e, on (1) or (2). These points are responsible for the change in the width of the street and are called **support vectors**. Once the support vectors are calculated in the training phase we only need these vectors to classify new datapoints during the prediction phase, hence reducing the computational load significantly. Equation (4) is a constraint in the optimization process of maximizing the street width $\\frac{2}{||{\\vec{w}}||}$. In the next section let us see how we can combine the optimization problem and the constraints together into a single optimization equation using the concept of  lagrange multipliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lagrange Multipliers and the Primal and Dual form <a id=\"primal\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machines are trying to solve the optimization problem of maximizing the street width $\\frac{2}{||{\\vec{w}}||}$ (which is equivalent to minimizing $\\frac{||w||^2}{2}$) with the contraint $y_{i}(\\vec{w}\\cdot\\vec{x}_{i} + b) \\geq 1$. This can be elegantly written in a single equation with the help of [lagrange multipliers](https://en.wikipedia.org/wiki/Lagrange_multiplier). The resulting equation to be minimized is called the **primal form** (6).\n",
    "\n",
    "**Primal form:** $$ L_{p} = \\frac{||w||^2}{2} - \\sum_{i}{\\alpha_{i}[y_{i}(\\vec{w}\\cdot\\vec{x}_{i} + b) -1]}\\tag{6}$$\n",
    "\n",
    "$$\\frac{{\\partial L}}{\\partial \\vec{w}} = \\vec{w} - \\sum_{i}{\\alpha_{i}y_{i}\\vec{x_{i}}}$$\n",
    "\n",
    "equating $\\frac{{\\partial L}}{\\partial \\vec{w}}$ to 0 we get, \n",
    "\n",
    "$$ \\vec{w} = \\sum_{i}{\\alpha_{i}y_{i}\\vec{x_{i}}}\\tag{7}$$\n",
    "\n",
    "$$\\frac{{\\partial L}}{\\partial \\vec{b}} = \\sum_{i}{\\alpha_{i}y_{i}}$$\n",
    "\n",
    "and equating $\\frac{{\\partial L}}{\\partial \\vec{b}}$ to 0 we convert the primal form to the dual form, \n",
    "\n",
    "$$\\sum_{i}{\\alpha_{i}y_{i}} = 0\\tag{8}$$\n",
    "\n",
    "$$L = \\frac{1}{2}(\\sum_{i}{\\alpha_{i}y_{i}\\vec{x_{i}}})(\\sum_{j}{\\alpha_{j}y_{j}\\vec{x_{j}}}) - (\\sum_{i}{\\alpha_{i}y_{i}\\vec{x_{i}}})(\\sum_{j}{\\alpha_{j}y_{j}\\vec{x_{j}}}) - \\sum_{i}{\\alpha_{i}y_{i}b} + \\sum_{i}{\\alpha_{i}}$$\n",
    "\n",
    "**Dual form:** $$L_{d} = \\sum_{i}{\\alpha_{i}} - \\frac{1}{2}\\sum_{i}\\sum_{j}\\alpha_{i}\\alpha_{j}y_{i}y_{j}(\\vec{x}_{i}\\cdot\\vec{x}_{j})\\tag{9}$$\n",
    "subject to: $$\\sum_{i}{\\alpha_{i}y_{i}} = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a closer look at the dual form $L_{d}$ we can see that it is a function quadratic in the lagrange multipler terms which can be solved efficiently on a classical computer using [quadratic programming](https://en.wikipedia.org/wiki/Quadratic_programming) techniques. However, Note that finding the dot product $\\vec{x}_{i}\\cdot\\vec{x}_{j}$ becomes computationally expensive as the dimension of our data increases. In the days to come we'll learn how a quantum computer could be used to classify a classical dataset using an algorithm called the Variational Quantum Classifier (VQC) Algorithm as given in [this paper](https://arxiv.org/abs/1804.11326). Understanding of Classical SVM may not be required, however, some of the concepts such as kernels and feature maps will be crucial in understanding the VQC algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Prediction for a New Datapoint <a id=\"pred\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the training step are values of lagrange multipliers. Now, when a new datapoint $\\vec z$ is given lets see how we can find the classification result corresponding to it: \n",
    "\n",
    "* Step 1: Use the obtained values of lagrange multipliers to calculate the value of $\\vec{w}$ using $(7)$.\n",
    "* Step 2: Substitute the value of $\\vec{w}$ in equation $(5)$ and substitute a support vector in the place of $\\vec{x}_{i}$ to find the value of $b$.\n",
    "* Step 3: Find the value of $\\vec{w}\\cdot\\vec{z} + b$. If it $>0$ then assign $\\vec{z}$ a label $y_{z} = 1$ and $y_{z} = -1$ if the obtained value is $< 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing With Non-Linearly Separable Data <a id=\"non-linear\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes when the data is not linear eg:circular, SVM fails to find a satisfactory linear classification model. However, if we cleverly introduce a new parameter $r$ such that $r = e^{-x^{2}}$ and using that as our third parameter construct a new dataset (see picture below), we'll observe that a plane can be drawn horizontally passing through, say, $r=0.7$ to classify the dataset! This method in which we are mapping our dataset into a higher dimension to be able to find a linear boundary in the higher dimension is called a **feature map**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"http://quantdare.com/wp-content/uploads/2016/09/sepplane2.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"http://quantdare.com/wp-content/uploads/2016/09/svm_3d.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"http://quantdare.com/wp-content/uploads/2016/09/sepplan3.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "display(Image(url= \"http://quantdare.com/wp-content/uploads/2016/09/sepplane2.png\"))\n",
    "display(Image(url= \"http://quantdare.com/wp-content/uploads/2016/09/svm_3d.png\"))\n",
    "display(Image(url= \"http://quantdare.com/wp-content/uploads/2016/09/sepplan3.png\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Map and Kernel <a id=\"kernel\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen earlier a **feature map** maps our (non-linearly separable) input data to a higher dimensional  **feature space** where our data is now linearly separable. This helps circumvent the problem of dealing with non-linearly separable data, however, a new problem arises. As we keep increasing the dimension of our data, computing the coordinates of our data and the dot product $\\phi(\\vec{x}_{i})\\cdot\\phi(\\vec{x}_{j})$ in this higher dimentional feature space becomes computationally expensive. This is where the idea of the [Kernel functions](https://en.wikipedia.org/wiki/Kernel_method) comes in. \n",
    "\n",
    "Kernel functions allow us to deal with our data in the higher dimensional feature space (where our data is linearly separable) without ever having to compute the dot product in that space. \n",
    "\n",
    "if $\\phi(\\vec{x})$ is the feature map, then the corresponding kernel function is the dot product $\\phi(\\vec{x}_{i})\\cdot\\phi(\\vec{x}_{j})$, therefore, the kernel function $k$ is \n",
    "\n",
    "$$k(x_{i},x_{j}) = \\phi(\\vec{x}_{i})\\cdot\\phi(\\vec{x}_{j})$$\n",
    "\n",
    "Therefore, the corresponding transformed optimization problem can we written as, \n",
    "\n",
    "**Primal form:** $$ L_{p} = \\frac{||w||^2}{2} - \\sum_{i}{\\alpha_{i}[y_{i}(\\vec{w}\\cdot\\phi(\\vec{x}_{i}) + b) -1]}\\tag{6}$$\n",
    "\n",
    "**Dual form:** $$L_{d} = \\sum_{i}{\\alpha_{i}} - \\frac{1}{2}\\sum_{i}\\sum_{j}\\alpha_{i}\\alpha_{j}y_{i}y_{j}(\\phi(\\vec{x}_{i})\\cdot\\phi(\\vec{x}_{j}))$$\n",
    "or $$L_{d} = \\sum_{i}{\\alpha_{i}} - \\frac{1}{2}\\sum_{i}\\sum_{j}\\alpha_{i}\\alpha_{j}y_{i}y_{j}k(x_{i},x_{j})$$\n",
    "subject to: $$\\sum_{i}{\\alpha_{i}y_{i}} = 0$$\n",
    "where $$ \\vec{w} = \\sum_{i}{\\alpha_{i}y_{i}\\phi(\\vec{x_{i}})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rbf kernel is written as, \n",
    "\n",
    "$$k(x_{i},x_{j}) = exp(-||x_{i} - x_{j}||^{2}/2\\sigma^{2}) $$\n",
    "\n",
    "where $\\sigma$ is a tunable parameter \n",
    "\n",
    "What we should understand here is that the rbf kernel projects our data into an infinite dimensional feature space, however, the computational power required to compute the kernel function's value is quite negligible! As you see, we don't have to compute the dot product of the infinite dimensional vectors. This is how kernels help SVMs tackle non-linearly separable data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook is inspired from the resources from Qiskit India Challenge 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources: \n",
    "1. MIT Open Courseware lecture: https://youtu.be/_PwhiWxHK8o \n",
    "3. MIT lecture slides: http://web.mit.edu/6.034/wwwbob/svm-notes-long-08.pdf\n",
    "4. SVM Wikipedia page : https://en.wikipedia.org/wiki/Support_vector_machine\n",
    "2. SVM tutorial using sklearn: https://jakevdp.github.io/PythonDataScienceHandbook/05.07-support-vector-machines.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources <a id=\"add\"></a>\n",
    "1. Andrew NG notes: http://cs229.stanford.edu/notes/cs229-notes3.pdf\n",
    "2. Andrew NG lecture: https://youtu.be/lDwow4aOrtg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
